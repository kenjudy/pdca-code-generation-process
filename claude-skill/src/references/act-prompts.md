# ACT Phase Prompts

## Retrospective

**Purpose:** Analyze collaboration effectiveness and identify process improvements
**Duration:** 5-10 minutes
**Prerequisites:** Completed work session (successful or not)
**Next step:** Update working agreements and process templates based on learnings
**Critical:** Capture learnings while session details are fresh

Use the agent to help brainstorm ways to improve human/agent interaction:

```markdown
Let's retrospect on our coding session. Please address whichever of these areas seem most relevant:

**Session Overview:** What was our main goal and scope?

**Critical Moments Analysis:** 
- What were the 2-3 moments where our approach most impacted success/failure?
- What specific decisions or interventions were game-changers?

**Technical & Process Insights:** 
- What patterns in our collaboration most impacted effectiveness?
- What would have accelerated progress?
- What process elements worked well vs. need improvement?

**Collaboration Analysis:**
- Where did process discipline break down (if it did)?
- How effective was the "process police" intervention?
- What communication patterns helped vs. hindered progress?

**Top 3 Actionable Insights:**
1. **Start doing:** What prompting/guidance practice should you implement next time?
2. **Stop doing:** What setup/intervention approach should you eliminate?  
3. **Keep doing:** What guidance pattern was most valuable and should be protected?

**Next Session Setup Decisions:**
- **Process boundaries:** What rules should be non-negotiable vs. flexible?
- **Intervention triggers:** What specific behaviors should prompt immediate correction?  
- **Success metrics:** How will we know the process is working in the first 30 minutes?
- **Escape hatches:** When is it acceptable to deviate from the established process?
- **Discipline balance:** How can we maintain discipline while staying flexible?

Finally, **If you could only change ONE thing about our collaboration approach, what would it be and why?**
```

**Human's Commitment:** Review the results and determine no more than 1-3 small, incremental changes you might want to make to existing prompts or your behavior. Make the changes and follow through on them, then evaluate if they actually improve your output over the next few cycles.

## Framework Improvement Suggestions

Ask the agent to suggest targeted improvements to your prompts:

```markdown
Based on learnings would you suggest changes to this interaction framework. 

[paste in all or part of your template framework]

Only suggest targeted, specific and highly relevant changes.
```

## Skeptical Review

Filter suggestions to avoid unnecessary complexity:

```markdown
Review these suggested changes skeptically and remove any that are not going to materially improve outcomes.
```

## Implementation Guidelines Review

Periodically review working agreements:

```markdown
Implementation guidelines should not change much between sessions, but anything to consider changing here?

[paste your current working agreements]
```

## Continuous Improvement Process

**Keep track of:**
- Retrospective notes from each session
- Human context insights (what intervention timing worked best)
- Patterns in what prompting approaches are most effective
- Common failure modes and how to prevent them

**Summarize learnings into:**
- Updated working agreements
- Refined prompt templates
- New intervention triggers
- Better process checkpoints

**Iterate on:**
- Prompt templates based on what works
- Working agreements to reflect new insights
- Definition of done criteria
- Testing strategies

The point is what _you can do to better elicit desired behavior from the LLM next time._ Focus on your actions and prompts, not trying to change the fundamental nature of the AI agent.

## When to Run Retrospective

- After successful completion of a coding session
- After encountering significant challenges
- When you notice recurring patterns (good or bad)
- Before starting a new, complex feature
- At least once per day when actively using the framework

Regular retrospectives enable rapid evolution of your practices and prompts based on real feedback from actual usage.
